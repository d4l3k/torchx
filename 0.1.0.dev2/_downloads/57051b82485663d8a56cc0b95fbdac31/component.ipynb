{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torchx\n!wget --no-clobber https://github.com/pytorch/torchx/archive/refs/heads/master.tar.gz\n!tar xvf master.tar.gz torchx-master --strip-components=1\n\nNOTEBOOK = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Trainer Component Example\n\nThis is a component definition that runs the example lightning_classy_vision app.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Dict\n\nimport torchx.specs.api as torchx\nfrom torchx.components.base.binary_component import binary_component\nfrom torchx.specs import named_resources\n\n\ndef trainer(\n    image: str,\n    output_path: str,\n    data_path: str,\n    entrypoint: str = \"lightning_classy_vision/train.py\",\n    load_path: str = \"\",\n    log_path: str = \"/logs\",\n    resource: Optional[str] = None,\n    nnodes: int = 1,\n    env: Optional[Dict[str, str]] = None,\n    nproc_per_node: int = 1,\n    skip_export: bool = False,\n) -> torchx.AppDef:\n    \"\"\"Runs the example lightning_classy_vision app.\n\n    Args:\n        image: image to run (e.g. foobar:latest)\n        output_path: output path for model checkpoints (e.g. file:///foo/bar)\n        load_path: path to load pretrained model from\n        data_path: path to the data to load\n        log_path: path to save tensorboard logs to\n        resource: the resources to use\n        nnodes: number of nodes\n        env: env variables for the app\n        nproc_per_node: number of processes per node\n        skip_export: disable model export\n    \"\"\"\n    env = env or {}\n    args = [\n        \"--output_path\",\n        output_path,\n        \"--load_path\",\n        load_path,\n        \"--log_pat\",\n        log_path,\n        \"--data_path\",\n        data_path,\n    ]\n    if skip_export:\n        args.append(\"--skip_export\")\n    return binary_component(\n        name=\"examples-lightning_classy_vision-trainer\",\n        entrypoint=entrypoint,\n        args=args,\n        env=env,\n        image=image,\n        resource=named_resources[resource]\n        if resource\n        else torchx.Resource(cpu=1, gpu=0, memMB=1024),\n    )\n\n\ndef interpret(\n    image: str,\n    load_path: str,\n    data_path: str,\n    output_path: str,\n    resource: Optional[str] = None,\n) -> torchx.AppDef:\n    \"\"\"Runs the model interpretability app on the model outputted by the training\n    component.\n\n    Args:\n        image: image to run (e.g. foobar:latest)\n        load_path: path to load pretrained model from\n        data_path: path to the data to load\n        output_path: output path for model checkpoints (e.g. file:///foo/bar)\n        resource: the resources to use\n    \"\"\"\n    return binary_component(\n        name=\"examples-lightning_classy_vision-interpret\",\n        entrypoint=\"lightning_classy_vision/interpret.py\",\n        args=[\n            \"--load_path\",\n            load_path,\n            \"--data_path\",\n            data_path,\n            \"--output_path\",\n            output_path,\n        ],\n        image=image,\n        resource=named_resources[resource]\n        if resource\n        else torchx.Resource(cpu=1, gpu=0, memMB=1024),\n    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}