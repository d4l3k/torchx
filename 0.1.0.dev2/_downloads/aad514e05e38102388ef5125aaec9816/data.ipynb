{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torchx\n!wget --no-clobber https://github.com/pytorch/torchx/archive/refs/heads/master.tar.gz\n!tar xvf master.tar.gz torchx-master --strip-components=1\n\nNOTEBOOK = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Trainer Datasets Example\n\nThis is the datasets used for the training example. It's using stock Pytorch\nLightning + Classy Vision libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os.path\nimport tarfile\nfrom typing import Optional, Callable\n\nimport fsspec\nimport pytorch_lightning as pl\nfrom classy_vision.dataset.classy_dataset import ClassyDataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This uses classy vision to define a dataset that we will then later use in our\nPytorch Lightning data module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class TinyImageNetDataset(ClassyDataset):\n    \"\"\"\n    TinyImageNetDataset is a ClassyDataset for the tiny imagenet dataset.\n    \"\"\"\n\n    def __init__(self, data_path: str, transform: Callable[[object], object]) -> None:\n        batchsize_per_replica = 16\n        shuffle = False\n        num_samples = 1000\n        dataset = datasets.ImageFolder(data_path)\n        super().__init__(\n            # pyre-fixme[6]\n            dataset,\n            batchsize_per_replica,\n            shuffle,\n            transform,\n            num_samples,\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For easy of use, we define a lightning data module so we can reuse it across\nour trainer and other components that need to load data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# pyre-fixme[13]: Attribute `test_ds` is never initialized.\n# pyre-fixme[13]: Attribute `train_ds` is never initialized.\n# pyre-fixme[13]: Attribute `val_ds` is never initialized.\nclass TinyImageNetDataModule(pl.LightningDataModule):\n    \"\"\"\n    TinyImageNetDataModule is a pytorch LightningDataModule for the tiny\n    imagenet dataset.\n    \"\"\"\n\n    train_ds: TinyImageNetDataset\n    val_ds: TinyImageNetDataset\n    test_ds: TinyImageNetDataset\n\n    def __init__(self, data_dir: str, batch_size: int = 16) -> None:\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n\n    def setup(self, stage: Optional[str] = None) -> None:\n        # Setup data loader and transforms\n        img_transform = transforms.Compose(\n            [\n                transforms.Grayscale(),\n                transforms.ToTensor(),\n            ]\n        )\n        self.train_ds = TinyImageNetDataset(\n            data_path=os.path.join(self.data_dir, \"train\"),\n            transform=lambda x: (img_transform(x[0]), x[1]),\n        )\n        self.val_ds = TinyImageNetDataset(\n            data_path=os.path.join(self.data_dir, \"val\"),\n            transform=lambda x: (img_transform(x[0]), x[1]),\n        )\n        self.test_ds = TinyImageNetDataset(\n            data_path=os.path.join(self.data_dir, \"test\"),\n            transform=lambda x: (img_transform(x[0]), x[1]),\n        )\n\n    def train_dataloader(self) -> DataLoader:\n        # pyre-fixme[6]\n        return DataLoader(self.train_ds, batch_size=self.batch_size)\n\n    def val_dataloader(self) -> DataLoader:\n        # pyre-fixme[6]:\n        return DataLoader(self.val_ds, batch_size=self.batch_size)\n\n    def test_dataloader(self) -> DataLoader:\n        # pyre-fixme[6]\n        return DataLoader(self.test_ds, batch_size=self.batch_size)\n\n    def teardown(self, stage: Optional[str] = None) -> None:\n        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To pass data between the different components we use fsspec which allows us to\nread/write to cloud or local file storage.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def download_data(remote_path: str, tmpdir: str) -> str:\n    \"\"\"\n    download_data downloads the training data from the specified remote path via\n    fsspec and places it in the tmpdir unextracted.\n    \"\"\"\n    tar_path = os.path.join(tmpdir, \"data.tar.gz\")\n    print(f\"downloading dataset from {remote_path} to {tar_path}...\")\n    fs, _, rpaths = fsspec.get_fs_token_paths(remote_path)\n    assert len(rpaths) == 1, \"must have single path\"\n    fs.get(rpaths[0], tar_path)\n\n    data_path = os.path.join(tmpdir, \"data\")\n    print(f\"extracting {tar_path} to {data_path}...\")\n    with tarfile.open(tar_path, mode=\"r\") as f:\n        f.extractall(data_path)\n\n    return data_path"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}